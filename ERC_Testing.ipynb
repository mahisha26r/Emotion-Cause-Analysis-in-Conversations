{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476926f1-b1b5-44f0-9159-6a32305f4fc6",
   "metadata": {
    "id": "476926f1-b1b5-44f0-9159-6a32305f4fc6"
   },
   "source": [
    "# This is the reimplementation of the paper \"EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa\" by Taewoon Kim and Piek Vossen, Vrije Universiteit Amsterdam.\n",
    "\r",
    "# \n",
    "Link of the paper: https://arxiv.org/pdf/2108.12009.pdf\n",
    "\n",
    "# \r\n",
    "Few parts of the code are copied directly from https://github.com/tae898/erc and are mentioned seperately in the code with the help  f comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "691a6f0a-96a0-4ca6-95c9-5e6d59fee9f7",
   "metadata": {
    "id": "691a6f0a-96a0-4ca6-95c9-5e6d59fee9f7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ae3dba-1ab8-44c1-bb29-fe0df7edf50c",
   "metadata": {
    "id": "c3ae3dba-1ab8-44c1-bb29-fe0df7edf50c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 23:06:10.098226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 23:06:11.063967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f436b5c4-be69-40d1-a88a-37c48b6bb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('Subtask_1_train.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i].pop('conversation_ID')\n",
    "    data[i].pop('emotion-cause_pairs')\n",
    "\n",
    "new_data = []\n",
    "for i in range(len(data)):\n",
    "    new_data.append(data[i]['conversation'])\n",
    "\n",
    "emo_dict = {\"neutral\":0, \"joy\":1, \"surprise\":2, \"anger\":3, \"sadness\":4, \"disgust\":5, \"fear\":6}\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    for j in range(len(new_data[i])):\n",
    "        new_data[i][j].pop('utterance_ID')\n",
    "        new_data[i][j].update({\"utterance\": new_data[i][j][\"speaker\"]+\": \"+new_data[i][j][\"text\"]})\n",
    "        emotion = emo_dict[new_data[i][j]['emotion']]\n",
    "        new_data[i][j].update({'emotion':emotion})\n",
    "        new_data[i][j].pop('text')\n",
    "        new_data[i][j].pop('speaker')\n",
    "\n",
    "training_data = new_data[:1099].copy()\n",
    "testing_data = new_data[1236:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54750d90-c2b6-4a37-8238-b578c7506f53",
   "metadata": {
    "id": "54750d90-c2b6-4a37-8238-b578c7506f53"
   },
   "outputs": [],
   "source": [
    "class ErcDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, num_past_utterances=0, num_future_utterances=0):\n",
    "        \"\"\"Initializer for emotion recognition in conversation text dataset.\"\"\"\n",
    "        self.data = data\n",
    "        self.num_past_utterances = num_past_utterances\n",
    "        self.num_future_utterances = num_future_utterances\n",
    "        self.processed_data = self._create_input()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the processed data.\"\"\"\n",
    "        return len(self.processed_data)\n",
    "\n",
    "    def _create_input(self):\n",
    "        \"\"\"Creates inputs for RoBERTa.\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained('/home/research/shaina mehta/RoBBERTa/Project/Tokenizer', use_fast=True)\n",
    "        max_model_input_size = tokenizer.max_model_input_sizes['FacebookAI/roberta-base']\n",
    "        inputs = []\n",
    "        for dialog in self.data:\n",
    "            num_truncated = 0\n",
    "            for idx, utterance in enumerate(dialog):\n",
    "                num_tokens = len(tokenizer(utterance[\"utterance\"])[\"input_ids\"])\n",
    "                label = utterance[\"emotion\"]\n",
    "                indexes = [idx]\n",
    "                indexes_past = [i for i in range(idx - 1, idx - self.num_past_utterances - 1, -1)]\n",
    "                indexes_future = [i for i in range(idx + 1, idx + self.num_future_utterances + 1, 1)]\n",
    "                offset = 0\n",
    "                if len(indexes_past) < len(indexes_future):\n",
    "                    indexes_past.extend([None] * (len(indexes_future) - len(indexes_past)))\n",
    "                elif len(indexes_past) > len(indexes_future):\n",
    "                    indexes_future.extend([None] * (len(indexes_past) - len(indexes_future)))\n",
    "                for i, j in zip(indexes_past, indexes_future):\n",
    "                    if i is not None and i >= 0:\n",
    "                        indexes.insert(0, i)\n",
    "                        offset += 1\n",
    "                        if sum(num_tokens for idx_ in indexes) > max_model_input_size:\n",
    "                            del indexes[0]\n",
    "                            offset -= 1\n",
    "                            num_truncated += 1\n",
    "                            break\n",
    "                    if j is not None and j < len(dialog):\n",
    "                        indexes.append(j)\n",
    "                        if sum(num_tokens for idx_ in indexes) > max_model_input_size:\n",
    "                            del indexes[-1]\n",
    "                            num_truncated += 1\n",
    "                            break\n",
    "                final_utterance = \"</s></s>\".join([dialog[idx_][\"utterance\"] for idx_ in indexes])\n",
    "                input_ids_attention_mask = tokenizer(final_utterance, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_model_input_size)\n",
    "                input_ = {\n",
    "                    \"input_ids\": input_ids_attention_mask[\"input_ids\"].squeeze(),\n",
    "                    \"attention_mask\": input_ids_attention_mask[\"attention_mask\"].squeeze(),\n",
    "                    \"label\": label,\n",
    "                }\n",
    "                inputs.append(input_)\n",
    "        return inputs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns the processed data at the given index.\"\"\"\n",
    "        return self.processed_data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "244b43a8-b5ca-4175-baf5-b67ec8b14c0d",
   "metadata": {
    "id": "244b43a8-b5ca-4175-baf5-b67ec8b14c0d"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('/home/research/shaina mehta/RoBBERTa/Project/Model Weights', num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "541f2116-afca-4dd3-82a8-403604af6564",
   "metadata": {
    "id": "541f2116-afca-4dd3-82a8-403604af6564"
   },
   "outputs": [],
   "source": [
    "training_dataset = ErcDataset(training_data,0,0)\n",
    "validation_dataset = ErcDataset(testing_data,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a86cb9e-1e79-4e1f-a745-eb16b0fde0f1",
   "metadata": {
    "id": "8a86cb9e-1e79-4e1f-a745-eb16b0fde0f1"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "f1 = evaluate.load(\"f1\")\n",
    "accuracy = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b35c657b-a655-4377-83bb-71e5a69ad0f2",
   "metadata": {
    "id": "b35c657b-a655-4377-83bb-71e5a69ad0f2"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_predictions):\n",
    "    preds = np.argmax(eval_predictions.predictions, axis=1)\n",
    "    return {\"f1 score\": f1.compute(predictions=preds, references=eval_predictions.label_ids, average='macro'), \n",
    "            \"accuracy\": accuracy.compute(predictions=preds, references=eval_predictions.label_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "704eb2ac-f685-4d68-af7c-5433359f3aa4",
   "metadata": {
    "id": "704eb2ac-f685-4d68-af7c-5433359f3aa4"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/research/shaina mehta/RoBBERTa/Project/Tokenizer', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3e5a95-2a0c-4f0a-b7ef-730c0356d043",
   "metadata": {
    "id": "6a3e5a95-2a0c-4f0a-b7ef-730c0356d043",
    "outputId": "d2e22ae4-2f9b-470e-e76a-73be495caf66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data_dump/conda/miniconda3/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# taken reference from https://github.com/tae898/erc\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/research/shaina mehta/RoBBERTa/Project/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=40,\n",
    "    per_device_eval_batch_size=40,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy='epoch',\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c1c0e23-c02e-497e-a03c-b0976f08a6bc",
   "metadata": {
    "id": "2c1c0e23-c02e-497e-a03c-b0976f08a6bc",
    "outputId": "b8ada67a-be82-4c1c-e0be-a6607afc3076"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/37 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.097917079925537,\n",
       " 'eval_f1 score': {'f1': 0.43087254031226385},\n",
       " 'eval_accuracy': {'accuracy': 0.5803389830508474},\n",
       " 'eval_runtime': 10.8061,\n",
       " 'eval_samples_per_second': 136.496,\n",
       " 'eval_steps_per_second': 3.424}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c50914-df83-4b92-a1fe-266f5e46cd4b",
   "metadata": {
    "id": "87c50914-df83-4b92-a1fe-266f5e46cd4b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
